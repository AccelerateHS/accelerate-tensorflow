
#include <iostream>
#include <cstring>
#include <cstdint>

#include "tensorflow/lite/interpreter.h"
#include "tensorflow/lite/kernels/register.h"
#include "tensorflow/lite/model.h"
#include "tensorflow/lite/optional_debug_tools.h"

#include "edgetpu.h"

#if 0
#define DEBUG(...) std::cerr << __VA_ARGS__ << std::endl
#else
#define DEBUG(...) {}
#endif

// NOTE: Must match encoding generated by 'tagOfType'
//
typedef enum __attribute__((__packed__)) {
    TypeInt8   = 0,
    TypeInt16  = 1,
    TypeInt32  = 2,
    TypeInt64  = 3,
    TypeWord8  = 4,
    TypeWord16 = 5,
    TypeWord32 = 6,
    TypeWord64 = 7,
    TypeHalf   = 8,
    TypeFloat  = 9,
    TypeDouble = 10
} TensorType;

std::unique_ptr<tflite::Interpreter>
BuildEdgeTpuInterpreter
(
    const tflite::FlatBufferModel& model,
    edgetpu::EdgeTpuContext* edgetpu_context
)
{
  DEBUG("[edgetpu.cc] Creating resolver")

  tflite::ops::builtin::BuiltinOpResolver resolver;

  DEBUG("[edgetpu.cc] Adding custom op to resolver")

  resolver.AddCustom(edgetpu::kCustomOp, edgetpu::RegisterCustomOp());

  DEBUG("[edgetpu.cc] Invoking InterpreterBuilder")

  std::unique_ptr<tflite::Interpreter> interpreter;
  if (tflite::InterpreterBuilder(model, resolver)(&interpreter) != kTfLiteOk) {
    std::cerr << "Failed to build TPU interpreter" << std::endl;
    return nullptr;
  }

  DEBUG("[edgetpu.cc] Binding device context to interpreter");

  // Bind given context with interpreter
  interpreter->SetExternalContext(kTfLiteEdgeTpuContext, edgetpu_context);

  DEBUG("[edgetpu.cc] Setting interpreter threadcount");

  interpreter->SetNumThreads(1);

  DEBUG("[edgetpu.cc] Allocating tensors");

  if (interpreter->AllocateTensors() != kTfLiteOk) {
    std::cerr << "Failed to allocate TPU tensors" << std::endl;
    return nullptr;
  }

  return interpreter;
}

// Returns 0 on success, 1 on failure.
extern "C"
int64_t
edgetpu_run
(
    const char* model_buffer,
    const int64_t buffer_size,
    const char** tensor_name,
    uint8_t* tensor_type,
    uint8_t** tensor_data,
    int64_t* tensor_size_bytes,
    int64_t tensor_count
)
{
  DEBUG("[edgetpu.cc] Building tflite model from buffer");

  // Load the TPU model
  // std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(model_path);
  std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromBuffer(model_buffer, buffer_size);

  DEBUG("[edgetpu.cc] Getting device context");

  // Get EdgeTPU context
  std::shared_ptr<edgetpu::EdgeTpuContext> context = edgetpu::EdgeTpuManager::GetSingleton()->OpenDevice();

  DEBUG("[edgetpu.cc] Building interpreter");

  std::unique_ptr<tflite::Interpreter> interpreter = BuildEdgeTpuInterpreter(*model, context.get());
  if (!interpreter) {
    return 1;
  }

  // Push the data to each named input tensor
  auto input_tensors = interpreter->inputs();
  for (int64_t i = 0; i < (int64_t)input_tensors.size(); ++i) {
    auto input_name = interpreter->GetInputName(i);

    for (int64_t j = 0; j < tensor_count; ++j) {
      if (0 == strcmp(input_name, tensor_name[j])) {
        uint8_t *input_tensor = nullptr;
        switch ((TensorType) tensor_type[j]) {
          case TypeInt8:   input_tensor = (uint8_t*)interpreter->typed_input_tensor<int8_t>(i); break;
          case TypeInt16:  input_tensor = (uint8_t*)interpreter->typed_input_tensor<int16_t>(i); break;
          case TypeInt32:  input_tensor = (uint8_t*)interpreter->typed_input_tensor<int32_t>(i); break;
          case TypeInt64:  input_tensor = (uint8_t*)interpreter->typed_input_tensor<int64_t>(i); break;
          case TypeWord8:  input_tensor = (uint8_t*)interpreter->typed_input_tensor<uint8_t>(i); break;
          case TypeWord16: input_tensor = (uint8_t*)interpreter->typed_input_tensor<uint16_t>(i); break;
          case TypeWord32: input_tensor = (uint8_t*)interpreter->typed_input_tensor<uint32_t>(i); break;
          case TypeWord64: input_tensor = (uint8_t*)interpreter->typed_input_tensor<uint64_t>(i); break;
          case TypeHalf:   input_tensor = (uint8_t*)interpreter->typed_input_tensor<uint16_t>(i); break;
          case TypeFloat:  input_tensor = (uint8_t*)interpreter->typed_input_tensor<float>(i); break;
          case TypeDouble: input_tensor = (uint8_t*)interpreter->typed_input_tensor<double>(i); break;
        }
        if (input_tensor == nullptr) {
          std::cerr << "[edgetpu.cc] Input tensor is NULL for input tensor " << i << " (name '" << input_name << "')" << std::endl;
          return 1;
        }
        memcpy(input_tensor, tensor_data[j], tensor_size_bytes[j]);
        break;
      }
    }
  }

  // Run the interpreter
  interpreter->Invoke();

  // Extract the data from each output tensor
  auto output_tensors = interpreter->outputs();
  for (int64_t i = 0; i < (int64_t)output_tensors.size(); ++i) {
    auto output_name = interpreter->GetOutputName(i);

    for (int64_t j = 0; j < tensor_count; ++j) {
      if (0 == strcmp(output_name, tensor_name[j])) {
        // Assumes that the size of the output tensor data is known statically,
        // but this is already required for tflite
        switch ((TensorType) tensor_type[j]) {
          case TypeInt8:   memcpy(tensor_data[j], interpreter->typed_output_tensor<int8_t>(i),   tensor_size_bytes[j]); break;
          case TypeInt16:  memcpy(tensor_data[j], interpreter->typed_output_tensor<int16_t>(i),  tensor_size_bytes[j]); break;
          case TypeInt32:  memcpy(tensor_data[j], interpreter->typed_output_tensor<int32_t>(i),  tensor_size_bytes[j]); break;
          case TypeInt64:  memcpy(tensor_data[j], interpreter->typed_output_tensor<int64_t>(i),  tensor_size_bytes[j]); break;
          case TypeWord8:  memcpy(tensor_data[j], interpreter->typed_output_tensor<uint8_t>(i),  tensor_size_bytes[j]); break;
          case TypeWord16: memcpy(tensor_data[j], interpreter->typed_output_tensor<uint16_t>(i), tensor_size_bytes[j]); break;
          case TypeWord32: memcpy(tensor_data[j], interpreter->typed_output_tensor<uint32_t>(i), tensor_size_bytes[j]); break;
          case TypeWord64: memcpy(tensor_data[j], interpreter->typed_output_tensor<uint64_t>(i), tensor_size_bytes[j]); break;
          case TypeHalf:   memcpy(tensor_data[j], interpreter->typed_output_tensor<uint16_t>(i), tensor_size_bytes[j]); break;
          case TypeFloat:  memcpy(tensor_data[j], interpreter->typed_output_tensor<float>(i),    tensor_size_bytes[j]); break;
          case TypeDouble: memcpy(tensor_data[j], interpreter->typed_output_tensor<double>(i),   tensor_size_bytes[j]); break;
        }
        break;
      }
    }
  }

  return 0;
}

// vim: set sw=2 ts=2 et:
